{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d69998a",
   "metadata": {},
   "source": [
    "# Topic Modeling with Gensim\n",
    "\n",
    "This notebook demonstrates how to perform topic modeling on a corpus of documents using Gensim's Latent Dirichlet Allocation (LDA) implementation. We'll process text documents and extract meaningful topics from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d791ff94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jonathan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jonathan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jonathan\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import nltk\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "# Import gensim and other libraries\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short, stem_text\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00ba145e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 8 sample documents in the 'sample_docs' directory.\n"
     ]
    }
   ],
   "source": [
    "# Create sample documents for topic modeling\n",
    "sample_documents = [\n",
    "    \"\"\"\n",
    "    Machine learning is a method of data analysis that automates analytical model building. \n",
    "    It is a branch of artificial intelligence based on the idea that systems can learn from data, \n",
    "    identify patterns and make decisions with minimal human intervention. Machine learning algorithms \n",
    "    build a model based on sample data, known as training data, in order to make predictions or \n",
    "    decisions without being explicitly programmed to do so.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Deep learning is part of a broader family of machine learning methods based on artificial neural networks \n",
    "    with representation learning. Learning can be supervised, semi-supervised or unsupervised. Deep learning \n",
    "    architectures such as deep neural networks, deep belief networks, recurrent neural networks and \n",
    "    convolutional neural networks have been applied to fields including computer vision, speech recognition, \n",
    "    natural language processing, audio recognition, social network filtering, machine translation, \n",
    "    bioinformatics, drug design, medical image analysis, material inspection and board game programs.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Natural language processing is a subfield of linguistics, computer science, and artificial intelligence \n",
    "    concerned with the interactions between computers and human language, in particular how to program \n",
    "    computers to process and analyze large amounts of natural language data. The goal is a computer capable \n",
    "    of understanding the contents of documents, including the contextual nuances of the language within them.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Computer vision is an interdisciplinary scientific field that deals with how computers can gain \n",
    "    high-level understanding from digital images or videos. From the perspective of engineering, \n",
    "    it seeks to automate tasks that the human visual system can do. Computer vision is concerned with \n",
    "    the automatic extraction, analysis and understanding of useful information from a single image or \n",
    "    a sequence of images.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and \n",
    "    systems to extract knowledge and insights from structured and unstructured data. It uses techniques \n",
    "    and theories drawn from many fields within the context of mathematics, statistics, computer science, \n",
    "    and information science. Data science is a concept to unify statistics, data analysis, informatics, \n",
    "    and their related methods in order to understand and analyze actual phenomena with data.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Artificial intelligence is intelligence demonstrated by machines, as opposed to the natural intelligence \n",
    "    displayed by animals including humans. Leading AI textbooks define the field as the study of intelligent \n",
    "    agents, which are systems that perceive their environment and take actions that maximize their chances \n",
    "    of achieving their goals. Some popular accounts use the term artificial intelligence to describe machines \n",
    "    that mimic cognitive functions that humans associate with the human mind, such as learning and problem solving.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Robotics is an interdisciplinary branch of engineering and science that includes mechanical engineering, \n",
    "    electrical engineering, computer science, and others. Robotics deals with the design, construction, \n",
    "    operation, and use of robots, as well as computer systems for their control, sensory feedback, \n",
    "    and information processing. These technologies are used to develop machines that can substitute for humans. \n",
    "    Robots are used in many situations that are dangerous or unpleasant to humans.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Internet of things is a system of interrelated computing devices, mechanical and digital machines, \n",
    "    objects, animals or people that are provided with unique identifiers and the ability to transfer \n",
    "    data over a network without requiring human-to-human or human-to-computer interaction. IoT has \n",
    "    evolved from wireless sensor networks and now encompasses a wide range of applications.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Save sample documents to files\n",
    "os.makedirs(\"sample_docs\", exist_ok=True)\n",
    "for i, doc in enumerate(sample_documents):\n",
    "    with open(f\"sample_docs/doc_{i+1}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(doc)\n",
    "\n",
    "print(f\"Created {len(sample_documents)} sample documents in the 'sample_docs' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b298d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TopicModeler class (simplified version of the script)\n",
    "class TopicModeler:\n",
    "    \"\"\"A class to perform topic modeling on text documents.\"\"\"\n",
    "    \n",
    "    def __init__(self, language='english'):\n",
    "        \"\"\"\n",
    "        Initialize the TopicModeler.\n",
    "        \n",
    "        Args:\n",
    "            language (str): Language for stopwords and tokenization\n",
    "        \"\"\"\n",
    "        self.language = language\n",
    "        try:\n",
    "            self.stop_words = set(nltk.corpus.stopwords.words(language))\n",
    "        except LookupError:\n",
    "            nltk.download('stopwords')\n",
    "            self.stop_words = set(nltk.corpus.stopwords.words(language))\n",
    "        \n",
    "        self.documents = []\n",
    "        self.processed_docs = []\n",
    "        self.dictionary = None\n",
    "        self.corpus = None\n",
    "        self.lda_model = None\n",
    "        \n",
    "    def load_documents(self, input_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Load documents from a directory or file.\n",
    "        \n",
    "        Args:\n",
    "            input_path (str): Path to directory containing text files or a single file\n",
    "        \"\"\"\n",
    "        print(f\"Loading documents from {input_path}\")\n",
    "        \n",
    "        path = Path(input_path)\n",
    "        \n",
    "        if path.is_file():\n",
    "            # Single file\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                self.documents = [content]\n",
    "        elif path.is_dir():\n",
    "            # Directory of files\n",
    "            file_patterns = ['*.txt', '*.md', '*.html', '*.htm']\n",
    "            files = []\n",
    "            for pattern in file_patterns:\n",
    "                files.extend(glob.glob(str(path / '**' / pattern), recursive=True))\n",
    "            \n",
    "            print(f\"Found {len(files)} files\")\n",
    "            \n",
    "            for file_path in files:\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                        self.documents.append(content)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file_path}: {e}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Input path {input_path} is neither a file nor a directory\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.documents)} documents\")\n",
    "    \n",
    "    def preprocess_documents(self, custom_filters: list = None) -> None:\n",
    "        \"\"\"\n",
    "        Preprocess documents for topic modeling.\n",
    "        \n",
    "        Args:\n",
    "            custom_filters (list): Custom list of words to filter out\n",
    "        \"\"\"\n",
    "        print(\"Preprocessing documents\")\n",
    "        \n",
    "        # Default Gensim filters\n",
    "        default_filters = [\n",
    "            strip_tags,\n",
    "            strip_punctuation,\n",
    "            strip_multiple_whitespaces,\n",
    "            strip_numeric,\n",
    "            remove_stopwords,\n",
    "            strip_short,\n",
    "            stem_text\n",
    "        ]\n",
    "        \n",
    "        # Process each document\n",
    "        self.processed_docs = []\n",
    "        for doc in self.documents:\n",
    "            # Apply Gensim preprocessing\n",
    "            processed = preprocess_string(doc, default_filters)\n",
    "            self.processed_docs.append(processed)\n",
    "        \n",
    "        print(f\"Preprocessed {len(self.processed_docs)} documents\")\n",
    "    \n",
    "    def create_dictionary_and_corpus(self) -> None:\n",
    "        \"\"\"Create dictionary and corpus for LDA.\"\"\"\n",
    "        print(\"Creating dictionary and corpus\")\n",
    "        \n",
    "        # Create dictionary\n",
    "        self.dictionary = corpora.Dictionary(self.processed_docs)\n",
    "        \n",
    "        # Filter extremes (optional)\n",
    "        self.dictionary.filter_extremes(no_below=2, no_above=0.8)\n",
    "        \n",
    "        # Create corpus\n",
    "        self.corpus = [self.dictionary.doc2bow(doc) for doc in self.processed_docs]\n",
    "        \n",
    "        print(f\"Dictionary size: {len(self.dictionary)}\")\n",
    "        print(f\"Corpus size: {len(self.corpus)}\")\n",
    "    \n",
    "    def train_lda_model(self, num_topics: int = 10, passes: int = 10, \n",
    "                       alpha: str = 'auto', eta: str = 'auto') -> None:\n",
    "        \"\"\"\n",
    "        Train the LDA model.\n",
    "        \n",
    "        Args:\n",
    "            num_topics (int): Number of topics to extract\n",
    "            passes (int): Number of passes through the corpus\n",
    "            alpha (str): Document-topic density parameter\n",
    "            eta (str): Topic-word density parameter\n",
    "        \"\"\"\n",
    "        print(f\"Training LDA model with {num_topics} topics\")\n",
    "        \n",
    "        if not self.corpus or not self.dictionary:\n",
    "            raise ValueError(\"Dictionary and corpus must be created before training the model\")\n",
    "        \n",
    "        # Train LDA model\n",
    "        self.lda_model = LdaModel(\n",
    "            corpus=self.corpus,\n",
    "            id2word=self.dictionary,\n",
    "            num_topics=num_topics,\n",
    "            passes=passes,\n",
    "            alpha=alpha,\n",
    "            eta=eta,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(\"LDA model training completed\")\n",
    "    \n",
    "    def print_topics(self, num_words: int = 10) -> None:\n",
    "        \"\"\"\n",
    "        Print the topics in a readable format.\n",
    "        \n",
    "        Args:\n",
    "            num_words (int): Number of words to show per topic\n",
    "        \"\"\"\n",
    "        if not self.lda_model:\n",
    "            raise ValueError(\"Model must be trained before printing topics\")\n",
    "        \n",
    "        topics = self.lda_model.print_topics(num_words=num_words)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TOPICS IDENTIFIED\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        for i, topic in enumerate(topics):\n",
    "            print(f\"\\nTopic {i + 1}:\")\n",
    "            print(topic[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2879dd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from sample_docs\n",
      "Found 8 files\n",
      "Loaded 8 documents\n",
      "Preprocessing documents\n",
      "Preprocessed 8 documents\n",
      "Creating dictionary and corpus\n",
      "Dictionary size: 42\n",
      "Corpus size: 8\n",
      "Training LDA model with 5 topics\n",
      "LDA model training completed\n",
      "\n",
      "==================================================\n",
      "TOPICS IDENTIFIED\n",
      "==================================================\n",
      "\n",
      "Topic 1:\n",
      "0.133*\"engin\" + 0.074*\"human\" + 0.070*\"scienc\" + 0.069*\"deal\" + 0.069*\"interdisciplinari\" + 0.069*\"us\" + 0.069*\"inform\" + 0.038*\"machin\" + 0.038*\"mechan\" + 0.038*\"branch\"\n",
      "\n",
      "Topic 2:\n",
      "0.143*\"learn\" + 0.108*\"network\" + 0.073*\"data\" + 0.073*\"machin\" + 0.056*\"base\" + 0.038*\"method\" + 0.038*\"analysi\" + 0.038*\"artifici\" + 0.038*\"program\" + 0.021*\"field\"\n",
      "\n",
      "Topic 3:\n",
      "0.151*\"data\" + 0.122*\"scienc\" + 0.064*\"method\" + 0.064*\"field\" + 0.064*\"us\" + 0.035*\"system\" + 0.035*\"algorithm\" + 0.035*\"order\" + 0.035*\"analysi\" + 0.035*\"process\"\n",
      "\n",
      "Topic 4:\n",
      "0.107*\"comput\" + 0.103*\"human\" + 0.081*\"imag\" + 0.056*\"network\" + 0.056*\"understand\" + 0.056*\"digit\" + 0.056*\"vision\" + 0.031*\"data\" + 0.031*\"interact\" + 0.031*\"machin\"\n",
      "\n",
      "Topic 5:\n",
      "0.123*\"intellig\" + 0.083*\"human\" + 0.083*\"languag\" + 0.064*\"artifici\" + 0.064*\"natur\" + 0.044*\"machin\" + 0.044*\"includ\" + 0.044*\"goal\" + 0.044*\"process\" + 0.044*\"comput\"\n"
     ]
    }
   ],
   "source": [
    "# Perform topic modeling on the sample documents\n",
    "modeler = TopicModeler()\n",
    "\n",
    "# Load documents\n",
    "modeler.load_documents(\"sample_docs\")\n",
    "\n",
    "# Preprocess documents\n",
    "modeler.preprocess_documents()\n",
    "\n",
    "# Create dictionary and corpus\n",
    "modeler.create_dictionary_and_corpus()\n",
    "\n",
    "# Train LDA model\n",
    "modeler.train_lda_model(num_topics=5, passes=20)\n",
    "\n",
    "# Print topics\n",
    "modeler.print_topics(num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c4bd63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x14cc9bec440>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeler.dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97bc7352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DOCUMENT TOPIC DISTRIBUTIONS\n",
      "==================================================\n",
      "\n",
      "Document 1:\n",
      "  Topic 2: 0.9926\n",
      "\n",
      "Document 2:\n",
      "  Topic 2: 0.9934\n",
      "\n",
      "Document 3:\n",
      "  Topic 5: 0.9922\n",
      "\n",
      "Document 4:\n",
      "  Topic 1: 0.2223\n",
      "  Topic 4: 0.7723\n",
      "\n",
      "Document 5:\n",
      "  Topic 3: 0.9928\n"
     ]
    }
   ],
   "source": [
    "# Analyze topic distribution for individual documents\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DOCUMENT TOPIC DISTRIBUTIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i in range(min(5, len(modeler.corpus))):  # Show first 5 documents\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    doc_topics = modeler.lda_model.get_document_topics(modeler.corpus[i])\n",
    "    for topic_id, prob in doc_topics:\n",
    "        print(f\"  Topic {topic_id+1}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d288ef8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, np.float32(0.99283814))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7852418",
   "metadata": {},
   "source": [
    "## Using the Standalone Script\n",
    "\n",
    "You can also use the standalone script we created for topic modeling. To use it from the command line:\n",
    "\n",
    "```bash\n",
    "python src/pydocs/topic_modeling.py --input_dir sample_docs --num_topics 5 --passes 20\n",
    "```\n",
    "\n",
    "This approach is useful for processing larger datasets or when you want to run topic modeling as a batch process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3ea038",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to perform topic modeling using Gensim's LDA implementation. We:\n",
    "\n",
    "1. Created sample documents on various technology topics\n",
    "2. Implemented a TopicModeler class to handle the topic modeling process\n",
    "3. Preprocessed the documents using Gensim's text preprocessing utilities\n",
    "4. Created a dictionary and corpus for LDA\n",
    "5. Trained an LDA model to extract topics\n",
    "6. Analyzed the topic distributions in individual documents\n",
    "\n",
    "This approach can be applied to any corpus of text documents to discover hidden thematic structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9951e2",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "To further improve the topic modeling results, you could:\n",
    "\n",
    "1. Experiment with different numbers of topics\n",
    "2. Adjust the preprocessing steps (e.g., add custom stop words)\n",
    "3. Try different algorithms like Latent Semantic Analysis (LSA) or Hierarchical Dirichlet Process (HDP)\n",
    "4. Evaluate model quality using coherence scores\n",
    "5. Visualize topics using tools like pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "052759f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28d48f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\001-trivial\\\\minimal-document.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\002-trivial-libre-office-writer\\\\002-trivial-libre-office-writer.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\003-pdflatex-image\\\\pdflatex-image.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\004-pdflatex-4-pages\\\\pdflatex-4-pages.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\005-libreoffice-writer-password\\\\libreoffice-writer-password.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\006-pdflatex-outline\\\\pdflatex-outline.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\007-imagemagick-images\\\\imagemagick-ASCII85Decode.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\007-imagemagick-images\\\\imagemagick-CCITTFaxDecode.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\007-imagemagick-images\\\\imagemagick-images.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\007-imagemagick-images\\\\imagemagick-lzw.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\008-reportlab-inline-image\\\\inline-image.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\009-pdflatex-geotopo\\\\GeoTopo-komprimiert.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\009-pdflatex-geotopo\\\\GeoTopo.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\010-pdflatex-forms\\\\pdflatex-forms.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\011-google-doc-document\\\\google-doc-document.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\012-libreoffice-form\\\\libreoffice-form.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\013-reportlab-overlay\\\\reportlab-overlay.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\014-outlines\\\\mistitled_outlines_example.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\015-arabic\\\\habibi-oneline-cmap.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\015-arabic\\\\habibi-rotated.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\015-arabic\\\\habibi.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\016-libre-office-link\\\\libre-office-link.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\017-unreadable-meta-data\\\\unreadablemetadata.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\018-base64-image\\\\base64image.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\019-grayscale-image\\\\grayscale-image.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\020-xmp\\\\output_with_metadata_pymupdf.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\021-pdfa\\\\crazyones-pdfa.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\022-pdfkit\\\\pdfkit.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\023-cmyk-image\\\\cmyk-image.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\024-annotations\\\\annotated_pdf.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\025-attachment\\\\with-attachment.pdf', 'C:\\\\Users\\\\Jonathan\\\\Desktop\\\\projects\\\\pydocs\\\\dev\\\\sample-pdfs\\\\026-latex-multicolumn\\\\multicolumn.pdf']\n"
     ]
    }
   ],
   "source": [
    "# Define the directory to search (current directory in this example)\n",
    "directory_path = r\"C:\\Users\\Jonathan\\Desktop\\projects\\pydocs\\dev\\sample-pdfs\"\n",
    "# Define the desired file extension\n",
    "file_extension = 'pdf'\n",
    "\n",
    "# Construct the glob pattern\n",
    "pattern = os.path.join(directory_path, f'**/*.{file_extension}')\n",
    "\n",
    "# Get a list of files matching the pattern\n",
    "matching_files = glob.glob(pattern, recursive=True)\n",
    "print(matching_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1df0d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading C:\\Users\\Jonathan\\Desktop\\projects\\pydocs\\dev\\sample-pdfs\\005-libreoffice-writer-password\\libreoffice-writer-password.pdf: File has not been decrypted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid parent xref., rebuild xref\n",
      "parsing for Object Streams\n",
      "Object 174 0 not defined.\n",
      "Object 172 0 not defined.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading C:\\Users\\Jonathan\\Desktop\\projects\\pydocs\\dev\\sample-pdfs\\017-unreadable-meta-data\\unreadablemetadata.pdf: Invalid object in /Pages\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "for file_path in matching_files:\n",
    "    doc = {}\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            r = PdfReader(f)\n",
    "            doc[\"metadata\"] = r.metadata\n",
    "            doc[\"content\"] = [i.extract_text() for i in r.pages]\n",
    "        docs.append(doc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1120268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metadata': {'/Producer': 'pdfTeX-1.40.23',\n",
       "  '/Creator': 'TeX',\n",
       "  '/CreationDate': \"D:20220403180542+02'00'\",\n",
       "  '/ModDate': \"D:20220403180542+02'00'\",\n",
       "  '/Trapped': '/False',\n",
       "  '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3'},\n",
       " 'content': ['Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod\\ntempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero\\neos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea taki-\\nmata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur\\nsadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna\\naliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea\\nrebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit\\namet.\\n1']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b060b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = reader.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52cf22bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "LaTeX with hyperref\n",
      "pdfTeX-1.40.23\n",
      "2022-04-06 20:15:41+02:00\n",
      "2022-07-16 17:23:03-05:00\n"
     ]
    }
   ],
   "source": [
    "print(meta.title)\n",
    "print(meta.author)\n",
    "print(meta.subject)\n",
    "print(meta.creator)\n",
    "print(meta.producer)\n",
    "print(meta.creation_date)\n",
    "print(meta.modification_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89fdbe56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents\n",
      "1 Foo 2\n",
      "2 Bar 2\n",
      "3 Baz 2\n",
      "4 Foo 2\n",
      "5 Bar 3\n",
      "6 Baz 3\n",
      "7 Foo 3\n",
      "8 Bar 4\n",
      "9 Baz 4\n",
      "1\n",
      "1 Foo\n",
      "Hello, here is some text without a meaning. This text should show what a\n",
      "printed text will look like at this place. If you read this text, you will get no\n",
      "information. Really? Is there no information? Is there a difference between this\n",
      "text and some nonsense like “Huardest gefburn”? Kjift – not at all! A blind\n",
      "text like this gives you information about the selected font, how the letters are\n",
      "written and an impression of the look. This text should contain all letters of the\n",
      "alphabet and it should be written in of the original language. There is no need\n",
      "for special content, but the length of words should match the language. Hello,\n",
      "here is some text without a meaning. This text should show what a printed text\n",
      "will look like at this place. If you read this text, you will get no information.\n",
      "Really? Is there no information? Is there a difference between this text and\n",
      "some nonsense like “Huardest gefburn”? Kjift – not at all! A blind text like this\n",
      "gives you information about the selected font, how the letters are written and\n",
      "an impression of the look. This text should contain all letters of the alphabet\n",
      "and it should be written in of the original language. There is no need for special\n",
      "content, but the length of words should match the language.\n",
      "2 Bar\n",
      "Hello, here is some text without a meaning. This text should show what a\n",
      "printed text will look like at this place. If you read this text, you will get no\n",
      "information. Really? Is there no information? Is there a difference between this\n",
      "text and some nonsense like “Huardest gefburn”? Kjift – not at all! A blind\n",
      "text like this gives you information about the selected font, how the letters are\n",
      "written and an impression of the look. This text should contain all letters of\n",
      "the alphabet and it should be written in of the original language. There is no\n",
      "need for special content, but the length of words should match the language. 7\n",
      "3 Baz\n",
      "Hello, here is some text without a meaning. This text should show what a\n",
      "printed text will look like at this place. If you read this text, you will get no\n",
      "information. Really? Is there no information? Is there a difference between this\n",
      "text and some nonsense like “Huardest gefburn”? Kjift – not at all! A blind\n",
      "text like this gives you information about the selected font, how the letters are\n",
      "written and an impression of the look. This text should contain all letters of\n",
      "the alphabet and it should be written in of the original language. There is no\n",
      "need for special content, but the length of words should match the language. 5\n",
      "4 Foo\n",
      "Hello, here is some text without a meaning. This text should show what a\n",
      "printed text will look like at this place. If you read this text, you will get no\n",
      "information. Really? Is there no information? Is there a difference between this\n",
      "text and some nonsense like “Huardest gefburn”? Kjift – not at all! A blind\n",
      "text like this gives you information about the selected font, how the letters are\n",
      "2\n",
      "written and an impression of the look. This text should contain all letters of the\n",
      "alphabet and it should be written in of the original language. There is no need\n",
      "for special content, but the length of words should match the language. Hello,\n",
      "here is some text without a meaning. This text should show what a printed text\n",
      "will look like at this place. If you read this text, you will get no information.\n",
      "Really? Is there no information? Is there a difference between this text and\n",
      "some nonsense like “Huardest gefburn”? Kjift – not at all! A blind text like this\n",
      "gives you information about the selected font, how the letters are written and\n",
      "an impression of the look. This text should contain all letters of the alphabet\n",
      "and it should be written in of the original language. There is no need for special\n",
      "content, but the length of words should match the language.\n",
      "5 Bar\n",
      "Hello, here is some text without a meaning. This text should show what a\n",
      "printed text will look like at this place. If you read this text, you will get no\n",
      "information. Really? Is there no information? Is there a difference between this\n",
      "text and some nonsense like “Huardest gefburn”? Kjift – not at all! A blind\n",
      "text like this gives you information about the selected font, how the letters are\n",
      "written and an impression of the look. This text should contain all letters of\n",
      "the alphabet and it should be written in of the original language. There is no\n",
      "need for special content, but the length of words should match the language. 7\n",
      "6 Baz\n",
      "Hello, here is some text without a meaning. This text should show what a\n",
      "printed text will look like at this place. If you read this text, you will get no\n",
      "information. Really? Is there no information? Is there a difference between this\n",
      "text and some nonsense like “Huardest gefburn”? Kjift – not at all! A blind\n",
      "text like this gives you information about the selected font, how the letters are\n",
      "written and an impression of the look. This text should contain all letters of\n",
      "the alphabet and it should be written in of the original language. There is no\n",
      "need for special content, but the length of words should match the language. 5\n",
      "7 Foo\n",
      "Hello, here is some text without a meaning. This text should show what a\n",
      "printed text will look like at this place. If you read this text, you will get no\n",
      "information. Really? Is there no information? Is there a difference between this\n",
      "text and some nonsense like “Huardest gefburn”? Kjift – not at all! A blind\n",
      "text like this gives you information about the selected font, how the letters are\n",
      "written and an impression of the look. This text should contain all letters of the\n",
      "alphabet and it should be written in of the original language. There is no need\n",
      "for special content, but the length of words should match the language. Hello,\n",
      "here is some text without a meaning. This text should show what a printed text\n",
      "will look like at this place. If you read this text, you will get no information.\n",
      "Really? Is there no information? Is there a difference between this text and\n",
      "some nonsense like “Huardest gefburn”? Kjift – not at all! A blind text like this\n",
      "3\n",
      "gives you information about the selected font, how the letters are written and\n",
      "an impression of the look. This text should contain all letters of the alphabet\n",
      "and it should be written in of the original language. There is no need for special\n",
      "content, but the length of words should match the language.\n",
      "8 Bar\n",
      "Hello, here is some text without a meaning. This text should show what a\n",
      "printed text will look like at this place. If you read this text, you will get no\n",
      "information. Really? Is there no information? Is there a difference between this\n",
      "text and some nonsense like “Huardest gefburn”? Kjift – not at all! A blind\n",
      "text like this gives you information about the selected font, how the letters are\n",
      "written and an impression of the look. This text should contain all letters of\n",
      "the alphabet and it should be written in of the original language. There is no\n",
      "need for special content, but the length of words should match the language. 7\n",
      "9 Baz\n",
      "Hello, here is some text without a meaning. This text should show what a\n",
      "printed text will look like at this place. If you read this text, you will get no\n",
      "information. Really? Is there no information? Is there a difference between this\n",
      "text and some nonsense like “Huardest gefburn”? Kjift – not at all! A blind\n",
      "text like this gives you information about the selected font, how the letters are\n",
      "written and an impression of the look. This text should contain all letters of\n",
      "the alphabet and it should be written in of the original language. There is no\n",
      "need for special content, but the length of words should match the language. 5\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(i.extract_text()) for i in reader.pages]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydocs (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
